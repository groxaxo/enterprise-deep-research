# TODO: Get your E2B API key from https://e2b.dev/docs/getting-started/api-key
E2B_API_KEY=""
ANTHROPIC_API_KEY=""
FIRECRAWL_API_KEY=""
SEARCH_API=tavily
TAVILY_API_KEY=""
LANGCHAIN_API_KEY=""
GOOGLE_CLOUD_PROJECT=""
OPENAI_API_KEY=""
# Optional: Set to use local OpenAI-compatible endpoints (e.g., Ollama at http://localhost:11434/v1)
# OPENAI_BASE_URL=http://localhost:11434/v1
SAMBNOVA_API_KEY=""
# TODO: Get your Groq API key from https://console.groq.com/keys
GROQ_API_KEY=""

JINA_API_KEY=""

MAX_WEB_RESEARCH_LOOPS=3
FETCH_FULL_PAGE=True

LLM_PROVIDER=google
LLM_MODEL=gemini-2.5-pro

## LLM Providers ##

# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-7-sonnet
# LLM_MODEL=claude-3-7-sonnet-thinking

# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4.1
# LLM_MODEL=o3-mini
# LLM_MODEL=o3-mini-reasoning

## Local LLM Setup (Ollama, vLLM, etc.) ##
# To use local OpenAI-compatible endpoints:
# 1. Set OPENAI_BASE_URL to your local endpoint (e.g., http://localhost:11434/v1 for Ollama)
# 2. Set LLM_PROVIDER=openai
# 3. Set LLM_MODEL to match the model name in your local server (e.g., llama3.1:8b)
# 4. OPENAI_API_KEY can be set to "dummy-key" for local endpoints
# Example:
# LLM_PROVIDER=openai
# LLM_MODEL=llama3.1:8b
# OPENAI_API_KEY=dummy-key
# OPENAI_BASE_URL=http://localhost:11434/v1

# LLM_PROVIDER=groq
# LLM_MODEL=deepseek-r1-distill-llama-70b

# LLM_PROVIDER=sambnova
# LLM_MODEL=DeepSeek-V3-0324

## Activity generation configuration ##
ENABLE_ACTIVITY_GENERATION=true
ACTIVITY_VERBOSITY=medium
ACTIVITY_LLM_PROVIDER=google
ACTIVITY_LLM_MODEL=gemini-2.5-flash

## Telemetry Configuration (Disabled by default for local/privacy) ##
LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=""
LANGCHAIN_PROJECT=""
OTEL_SDK_DISABLED=true

SCRAPYBARA_API_KEY=""